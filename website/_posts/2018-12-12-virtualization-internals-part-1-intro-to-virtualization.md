---
layout: post
footer-space: true
current: blog
title:  "Virtualization Internals Part 1 - Intro to Virtualization"
page-title: "Virtualization Internals Part 1 - Intro to Virtualization"
categories: virtualization
excerpt_separator: <!--more-->
---
The purpose of this series of articles is to explain how x86 virtualization internally works. I find most of the information dispatched in acamedical work and reserch papers, which is pretty hard to understand for beginners, I will try to start from scratch and build knowledge as needed. This could be useful for understanding how virtualization works, or writing your own hypervisor or in other scenarios such as attacking hypervisors security.
<!--more-->
- Virtualization Internals Part 1 - Intro to Virtualization (current)
- [Virtualization Internals Part 2 - VMWare and Full Virtualization using Binary Translation](https://saferwall.com/blog/virtualization-internals-part-2-vmware-and-virtualization-using-binary-translation)
- Virtualization Internals Part 3 - Xen and Paravirtualization
## What is Virtualization ?
In philosophy, virtual means `something that is not real`. In computing, virtualization refers to the act of creating a virtual (rather than actual) version of something, including hardware platforms, storage devices, and network resources.
Virtualization is a broad concept, and there are different areas where we can make use of virtualization, let's give some examples:
* Process-level virtualzation: The main idea behind this form of virtualization is to achieve portability among different platforms. It consists of an application implemented on top of an OS like Java Virtual Machine. Programs which runs on such a VM are writen in high level language which will be compiled into an intermediate instructions which will be interepred during runtime. There is also another form of virtualization which I would like to place in here called `code vitualization`. This was the first type of virtualization which I ever encountered while doing reverse engineering. Code virtualization aims for protection against code tampering and cracking. It consists of converting your original code (for example x86 instructions) into virtual opcodes that will only be understood by an internal virtual machine. 
* Storage Virtualization: consists of presenting a logical view of physical storage resources to a host computer, so the user can pull the data from the integrated storage resources regardless how data is stored or where. It can be implemented at the host level using LVM (for instance in Linux), or at the device level using RAID, or at the network level with SAN for example.
* Network Virtualization: Integrate network hardware resources with software resources to provide users with virtualization technology of virtual network connection. It can be divided into VLAN and VPN.
* Operating system-level virtualization: also known as `containerization`, refers to an OS feature in which the kernel allows the existence of multiple isolated user-space instances. These instances have a limited view of resources such as connected devices, files or folders, network shares, etc. One example of containerization software is docker. In linux, docker takes advantages of the kernel features such as namespaces and cgroupes to provide isolated environment for applications.
* System Virtualization: It refers to the creation of a virtual machine that acts like a real computer with an operating system. For example, a computer that is running Windows 10 may host a virtual machine which runs Ubuntu, both running at the same time. Pretty cool, nah ? This type of virtuaization is what we would be discussing in detail during this article series. Here is a screen shot of Linux Kubuntu running on Windows 7 with VirtualBox.
<p align="center"> 
    <img src="https://upload.wikimedia.org/wikipedia/commons/7/7c/VirtualBox_screenshot.png" width="600px" height="auto" alt="Virtualization Vs Non-Virtualization">
</p>
All in all, virtualization provides a simple and consistent interface to complexe functions. Indeed, there is little or no need to understand the underlying complexity itself, just remember that virtualization is all about abstraction. 
## Why virtualize ?
The usage of this technology brings so many benefits. Let's illustrate that with a real life example. Usually a company use multiple tools:
- an issue tracking and project management software like `jira`.
- a version control repository for code like `gitlab`.
- a continous intergration software like `jenkins`.
- a mail server for their emails like `MS exchange server`.
- ...
Without virtualization, you would probably need multiple servers to host all these services, as some of them would requires Windows as a host, others would need Linux as their base OS. With virtalization, you can use one single server to host multiple virtual machine at the same time, which each runs on a different OS (like OSX, Linux, and Windows), this design allow servers to be consolidated into a single physical machine.
In addition to that, if the there is a failure on one of them, it does not bring down any others. Thus, this approach encourages easy maintainability and cost saving to enterprises. On top of that, separating those services in different VMs is considered as a security feature as it supports strong isolation, which means if an attacker gains control to one of the servers, he does not have access to everything.
One more advantage, with virtualization you can easily adjust hardware resources according to your needs. For instance, if you host a web application in a VM, and your website have a huge number of requests during a certain period of the day that it becomes difficult to handle the load, in such cases, you do not need to open the server and plug-in manually some more RAM or CPU, you can instead easily scale it up by going to your VM configuration and adjust it with more resources, you can even spawn a new VM to balance the load, and let's say that if your website during the night have less traffic, we would just scale it down by reducing the resources so other VMs in the server make use of it. This approach allows resources to be managed efficiently, rather than having a physical server with so many cores and RAM, but idling most of the tim knowing that an idle server still consumes power and resources!
Vitrualization also helps a lot in software testing, it makes life easier for a programmer who want to make sure his software is running flawlessly before it gets deployed to production. When a programmer commit some new code, a VM is created on the fly and a serie of tests runs, code get released only if all tests passed. Furthermore, in malware analysis, you have the opportunity to take snapshots of the VM making it easy to go back to a clean state in case of something goes wrong while analyzing the malware. 
Last but not least, a virtual machine can be migrated, meaning that it is easy to move an entire machine from one server to another even with different hardware. This helps for example when the hardware begins to experience faults or when you got some maintance to do. It takes some mouse clicks to move all your stack and configuration to another server with no downtime.
With that on mind, virtualization offers tremendous space/power/cost savings to compagnies.
## A bit of History
It may suprise you that the concept of virtualization started with IBM mainframes in the earlies of 1960s with the development of `CP/40`. IBM had been selling computers that supported and heavily used virtualization. In these early days of computing, virtualization softwares allowed multiple users, each running their own single-user operating system instance, to share the same costly mainframe hardware.
Virtual machines lost popularity with the increased sophistication of multi-user OSs, the rapid drop in hardware cost, and the corresponding proliferation of computers. By the 1980s, the industry had lost interest in virtualization and new computer architectures developed in the 1980s and 1990s did not include the necessary architectural support for virtualization.
The real revolution started in 1990 when VMware introduced its first virtualization solution for x86. In its wake other products followed: `Xen`, `KVM`, `VirtualBox`, `Hyper-V`, `Parallels`, and many others. Interest in virtualization exploded in recent years and it is now a fondamental part of cloud computing, cloud services like Windows Azure, Amazon Web Services and Google Cloud Platform became actually a multi-billion $ market industry thanks to virtualization.
## Introducing VMM/hypervisor
Before we go deeper into the details, let's define some few terms:
* Hypervisor or VMM (Virtual Machine Monitor) is a peace of software which creates the illusion of multiple (virtual) machines on the same physical hardware. These two terms (hypervisor and VMM) are typically treated as synonyms, but according to some people, there is a slight distinction between them.
    - A virtual machine monitor (VMM) is a software that manages CPU, memory, I/O devices, interrupt, and the instruction set on a given virtualized environment.
    - A hypervisor may refer to an operating system with the VMM. In this article series, we consider these terms to have identical meanings to represent a software for virtual machine. 
* Guest OS is the operating system which is running inside the virtual machine.
<p align="center"> 
    <img src="https://i.imgur.com/IxsQUYU.png" width="600px" height="auto" alt="Virtualization Vs Non-Virtualization">
</p>
## What does it take to create a hypervisor
To create a hypervisor, we need to make sure to boot a VM like real machines and install arbitrary operating systems on them, just as can be done on the real hardware. It is the task of the hypervisor to provide this illusion and to do it efficiently. There is 3 areas of the system which needs to be considered when writting hypervisors:
    1. CPU and memory virtualization (priviliged instructions, MMU).
    2. Platform virtualization (interrupts, timers, ...).
    3. IO devices virtualization (network, disk, bios, ...).
In fact, two computer scientists `Gerald Popek` and `Robert Goldberg`, published a seminal paper _Formal Requirements for Virtualizable Third Generation Architectures_ that defines exactly what conditions needs to satisfy in order to support virtualization efficiently, these requirements are broken into three parts:
* Fidelity: Programs running in a virtual environment run identically to running natively, barring differences in resource availability and timing.
* Performance:  An overwhelming majority of guest instructions are executed by the hardware without the intervention of the VMM.
* Safety: The VMM manages all hardware resources.
Let's dissect those three characteristics: by `fidelity`, software on the VMM, typically an OS and all its applications, should execute identically to how it would on real hardware (modulo timing effects). So if you download an ISO of Linux Debian, you should be able to boot it and play with all the applications as you do in a real hardware.
For `performance` to be good, most instructions executed by the guest OS should be run directly on the underlying physical hardware without the intervention of the VMM. Emulators for example (Like Bochs) simulates all of the underlying physical hardware like CPU and Memory, all represented using data structures in the program, and instruction execution involves a dispatch loop that calls appropriate procedures to update these data structures for each instruction, the good thing about emulation is that you can emulate code even if it is writen for a different CPU, the disadvantage is that it is obviously slow. Thus, you cannot achieve good performance if are going to emulate all the instruction set, in other words, only privileged instructions should require the intervention of the VMM.
Finally, by `safety` it is important to protect data and resources on each virtual environment from any threats or performance interference in sharing physical resources. For example, if you assign a VM 1GB of RAM, the guest should not be able to use more memory that what it is attributed to it. Also, a faulty process in one VM should not scribble the memory of another VM. In addition to that, the VMM should not allow the guest for instance to disable interrups for the entire machine or modify the page table mapping, otherwise, the integrity of the hypervisor could be exploited and this could allow some sort of arbitrary code execution on the host, or other guests running in the same server, making the whole server vulnerable.
An early technique for virtualization was called `trap and emulate`, it was so prevalent as to be considered the only practical method for virtualization. A trap is basically a localized exception/faul...